# -*- coding: utf-8 -*-
"""WordCountWithStopwords(2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KwMh5rVDxuBHurirIka8v37o_-IPh1r-
"""

# List of stopwords
stopwords = set(["the", "and", "of", "a", "to", "in", "is", "it"])

# Mapper function
def mapper(text):
    words = text.split()
    for word in words:
        # Check if the lowercase version of the word is not in stopwords
        if word.lower() not in stopwords:
            yield (word, 1)  # Emit word as a key and 1 as the value

# Reducer function
def reducer(word, counts):
    total_count = sum(counts)
    yield (word, total_count)  # Emit word as a key and the total count as the value

# Read input data from a file
input_file = "WordCountWithStopwords(2)_Input.txt"
input_data = []

with open(input_file, "r") as file:
    for line in file:
        input_data.append(line)

# Initialize a dictionary to store word counts
word_counts = {}

# Map and Reduce
for line in input_data:
    for word, count in mapper(line):
        if word not in word_counts:
            word_counts[word] = []  # Initialize an empty list for the word if it's not already in the dictionary
        word_counts[word].append(count)  # Append the count to the word's list

# Initialize a dictionary to store the final word counts
result = {}

# Iterate through the word counts and perform the reduce step
for word, counts in word_counts.items():
    for word, count in reducer(word, counts):
        result[word] = count  # Store the final word count

# Print the output
for word, count in result.items():
    print(f'"{word}" {count}')